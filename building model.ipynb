{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a40d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB1, MobileNet,MobileNetV2,NASNetMobile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler, TensorBoard, CSVLogger \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acc1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(base_model, model_name):\n",
    "   model = tf.keras.Sequential([base_model,\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.Dropout(0.2),\n",
    "                                 tf.keras.layers.Dense(2, activation=\"softmax\")                                     \n",
    "                                ])\n",
    "   print(model_name + \" Network Architecture:\")\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65940ed8-8dc3-4394-8764-aaeef7cf78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-mobile-no-top.h5\n",
      "19996672/19993432 [==============================] - 8s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb1_notop.h5\n",
      "27025408/27018416 [==============================] - 8s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Input shape definition\n",
    "InputShape = (224,224,3) \n",
    "\n",
    "# NASNetMobile base model\n",
    "NasNetBase = NASNetMobile(include_top=False,\n",
    "                          weights=\"imagenet\",\n",
    "                          input_shape=InputShape)\n",
    "NasNetBase.trainable = True  # Allow fine-tuning\n",
    "\n",
    "# EfficientNetB1 base model\n",
    "EfficientNetBase = EfficientNetB1(include_top=False,\n",
    "                                  weights=\"imagenet\",\n",
    "                                  input_shape=InputShape)\n",
    "EfficientNetBase.trainable = True  # Allow fine-tuning\n",
    "\n",
    "# MobileNetV2 base model\n",
    "MobileNetV2Base = MobileNetV2(include_top=False,\n",
    "                              weights=\"imagenet\",\n",
    "                              alpha=1.0,\n",
    "                              input_shape=InputShape)\n",
    "MobileNetV2Base.trainable = True  # Allow fine-tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acb2329-0d1b-46e7-935d-0e95f7718522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NasNetModel Network Architecture:\n",
      "EfficientNet Network Architecture:\n",
      "MobileNetV2 Network Architecture:\n"
     ]
    }
   ],
   "source": [
    "NasNet = create_model(NasNetBase,'NasNetModel')\n",
    "EfficientNet = create_model(EfficientNetBase, 'EfficientNet')\n",
    "MobileNetV2 = create_model(MobileNetV2Base, 'MobileNetV2')\n",
    "\n",
    "Models = [NasNet,EfficientNet, MobileNetV2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "393a7445-6f1f-4774-833b-71de9fdfba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_df = pd.read_csv('brain_mri_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f01115ad-a977-4bb6-9a46-79d1f2d1deaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TCGA_CS_4941_19960909</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TCGA_CS_4941_19960909</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TCGA_CS_4941_19960909</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TCGA_CS_4941_19960909</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TCGA_CS_4941_19960909</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3924</th>\n",
       "      <td>TCGA_HT_A61B_19991127</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3925</th>\n",
       "      <td>TCGA_HT_A61B_19991127</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3926</th>\n",
       "      <td>TCGA_HT_A61B_19991127</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3927</th>\n",
       "      <td>TCGA_HT_A61B_19991127</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>TCGA_HT_A61B_19991127</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>C:/Users/aisha/machine learning/lgg-mri-segmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3929 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 patient_id  \\\n",
       "0     TCGA_CS_4941_19960909   \n",
       "1     TCGA_CS_4941_19960909   \n",
       "2     TCGA_CS_4941_19960909   \n",
       "3     TCGA_CS_4941_19960909   \n",
       "4     TCGA_CS_4941_19960909   \n",
       "...                     ...   \n",
       "3924  TCGA_HT_A61B_19991127   \n",
       "3925  TCGA_HT_A61B_19991127   \n",
       "3926  TCGA_HT_A61B_19991127   \n",
       "3927  TCGA_HT_A61B_19991127   \n",
       "3928  TCGA_HT_A61B_19991127   \n",
       "\n",
       "                                             image_path  \\\n",
       "0     C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "1     C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "2     C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "3     C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "4     C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "...                                                 ...   \n",
       "3924  C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "3925  C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "3926  C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "3927  C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "3928  C:/Users/aisha/machine learning/lgg-mri-segmen...   \n",
       "\n",
       "                                              mask_path  diagnosis  \n",
       "0     C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "1     C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "2     C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "3     C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "4     C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "...                                                 ...        ...  \n",
       "3924  C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "3925  C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "3926  C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "3927  C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "3928  C:/Users/aisha/machine learning/lgg-mri-segmen...          0  \n",
       "\n",
       "[3929 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4333cec-0286-44f8-9e00-99c45c388abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3929 entries, 0 to 3928\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   image_path  3929 non-null   object\n",
      " 1   mask_path   3929 non-null   object\n",
      " 2   diagnosis   3929 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 92.2+ KB\n"
     ]
    }
   ],
   "source": [
    "brain_df_train = brain_df.drop(columns=['Unnamed:0', 'patient_id'], axis=1, errors='ignore')\n",
    "brain_df_train['diagnosis'] = brain_df['diagnosis'].astype(str)\n",
    "brain_df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f12c86-b317-4c93-b920-5aa8b2d14a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3006 validated image filenames belonging to 2 classes.\n",
      "Found 333 validated image filenames belonging to 2 classes.\n",
      "Found 590 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(brain_df_train, test_size=0.15)\n",
    "datagen = ImageDataGenerator(rescale=1./255., validation_split=0.1)\n",
    "\n",
    "# Training data generator\n",
    "train_generator = datagen.flow_from_dataframe(train,\n",
    "                                              directory='./',\n",
    "                                              x_col='image_path',\n",
    "                                              y_col='diagnosis',\n",
    "                                              subset='training',\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              target_size=(224, 224))\n",
    "\n",
    "# Validation data generator\n",
    "valid_generator = datagen.flow_from_dataframe(train,\n",
    "                                              directory='./',\n",
    "                                              x_col='image_path',\n",
    "                                              y_col='diagnosis',\n",
    "                                              subset='validation',\n",
    "                                              class_mode='categorical',\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              target_size=(224, 224))\n",
    "\n",
    "# Test data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_generator = test_datagen.flow_from_dataframe(test,\n",
    "                                                  directory='./',\n",
    "                                                  x_col='image_path',\n",
    "                                                  y_col='diagnosis',\n",
    "                                                  class_mode='categorical',\n",
    "                                                  batch_size=16,\n",
    "                                                  shuffle=False,\n",
    "                                                  target_size=(224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3da49657-ccf4-4415-b1b2-d25a993ff0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping to avoid overfitting\n",
    "earlystopping = EarlyStopping(monitor='val_loss', \n",
    "                              mode='min', \n",
    "                              verbose=1, \n",
    "                              patience=15)\n",
    "\n",
    "# Saving the best model weights\n",
    "checkpointer = ModelCheckpoint(filepath=\"ensemble-weights_{epoch:02d}-{val_loss:.2f}.hdf5\",\n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               mode='min',\n",
    "                               verbose=1,\n",
    "                               patience=10, \n",
    "                               min_delta=0.0001,\n",
    "                               factor=0.2)\n",
    "\n",
    "# TensorBoard for visualization\n",
    "tensorboard = TensorBoard(log_dir='logs', \n",
    "                          histogram_freq=1,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "\n",
    "callbacks = [checkpointer, earlystopping, reduce_lr, tensorboard, csv_logger]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a377b8e-50dc-4d68-a9f5-10953a0ac7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:   0%|                                                                           | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9799\n",
      "Epoch 00001: val_loss improved from inf to 14.42306, saving model to ensemble-weights_01-14.42.hdf5\n",
      "187/187 [==============================] - 498s 3s/step - loss: 0.0560 - accuracy: 0.9799 - val_loss: 14.4231 - val_accuracy: 0.8313\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9903\n",
      "Epoch 00002: val_loss improved from 14.42306 to 8.22198, saving model to ensemble-weights_02-8.22.hdf5\n",
      "187/187 [==============================] - 576s 3s/step - loss: 0.0311 - accuracy: 0.9903 - val_loss: 8.2220 - val_accuracy: 0.8062\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9873\n",
      "Epoch 00003: val_loss improved from 8.22198 to 2.43436, saving model to ensemble-weights_03-2.43.hdf5\n",
      "187/187 [==============================] - 523s 3s/step - loss: 0.0428 - accuracy: 0.9873 - val_loss: 2.4344 - val_accuracy: 0.8813\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9890\n",
      "Epoch 00004: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 632s 3s/step - loss: 0.0379 - accuracy: 0.9890 - val_loss: 79.5665 - val_accuracy: 0.6594\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0786 - accuracy: 0.9796\n",
      "Epoch 00005: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 538s 3s/step - loss: 0.0786 - accuracy: 0.9796 - val_loss: 7.6996 - val_accuracy: 0.8938\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9920\n",
      "Epoch 00006: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 589s 3s/step - loss: 0.0331 - accuracy: 0.9920 - val_loss: 10.5249 - val_accuracy: 0.7437\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0059 - accuracy: 0.9980\n",
      "Epoch 00007: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 644s 3s/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 3.6782 - val_accuracy: 0.8906\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9957\n",
      "Epoch 00008: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 620s 3s/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 12.8047 - val_accuracy: 0.6812\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9853\n",
      "Epoch 00009: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 558s 3s/step - loss: 0.0452 - accuracy: 0.9853 - val_loss: 25.3474 - val_accuracy: 0.6125\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9876\n",
      "Epoch 00010: val_loss did not improve from 2.43436\n",
      "187/187 [==============================] - 646s 3s/step - loss: 0.0437 - accuracy: 0.9876 - val_loss: 8.1450 - val_accuracy: 0.8438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:  33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                        | 1/3 [1:37:34<3:15:09, 5854.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0080 - accuracy: 0.9970\n",
      "Epoch 00001: val_loss improved from 2.43436 to 0.55438, saving model to ensemble-weights_01-0.55.hdf5\n",
      "187/187 [==============================] - 1040s 6s/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.5544 - val_accuracy: 0.8562\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9960\n",
      "Epoch 00002: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 1028s 5s/step - loss: 0.0112 - accuracy: 0.9960 - val_loss: 5.2652 - val_accuracy: 0.6594\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9980\n",
      "Epoch 00003: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 1034s 6s/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 2.1513 - val_accuracy: 0.7375\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0125 - accuracy: 0.9963\n",
      "Epoch 00004: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 988s 5s/step - loss: 0.0125 - accuracy: 0.9963 - val_loss: 3.4995 - val_accuracy: 0.6812\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9967\n",
      "Epoch 00005: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 891s 5s/step - loss: 0.0112 - accuracy: 0.9967 - val_loss: 192.7499 - val_accuracy: 0.3313\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9926\n",
      "Epoch 00006: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 969s 5s/step - loss: 0.0209 - accuracy: 0.9926 - val_loss: 130.2158 - val_accuracy: 0.3313\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9950 \n",
      "Epoch 00007: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 3195s 17s/step - loss: 0.0254 - accuracy: 0.9950 - val_loss: 60.6963 - val_accuracy: 0.6719\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9923\n",
      "Epoch 00008: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 973s 5s/step - loss: 0.0212 - accuracy: 0.9923 - val_loss: 0.7902 - val_accuracy: 0.3313\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9933\n",
      "Epoch 00009: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 994s 5s/step - loss: 0.0189 - accuracy: 0.9933 - val_loss: 0.6266 - val_accuracy: 0.6687\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9923\n",
      "Epoch 00010: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 987s 5s/step - loss: 0.0276 - accuracy: 0.9923 - val_loss: 0.7606 - val_accuracy: 0.6687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 2/3 [5:00:08<2:39:20, 9560.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9910\n",
      "Epoch 00001: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 471s 3s/step - loss: 0.0271 - accuracy: 0.9910 - val_loss: 0.9415 - val_accuracy: 0.8719\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9916\n",
      "Epoch 00002: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 471s 3s/step - loss: 0.0343 - accuracy: 0.9916 - val_loss: 1.1494 - val_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9913\n",
      "Epoch 00003: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 473s 3s/step - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.8266 - val_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9950\n",
      "Epoch 00004: val_loss did not improve from 0.55438\n",
      "187/187 [==============================] - 470s 3s/step - loss: 0.0132 - accuracy: 0.9950 - val_loss: 0.5768 - val_accuracy: 0.8625\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0046 - accuracy: 0.9980\n",
      "Epoch 00005: val_loss improved from 0.55438 to 0.41263, saving model to ensemble-weights_05-0.41.hdf5\n",
      "187/187 [==============================] - 465s 2s/step - loss: 0.0046 - accuracy: 0.9980 - val_loss: 0.4126 - val_accuracy: 0.9312\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9957\n",
      "Epoch 00006: val_loss did not improve from 0.41263\n",
      "187/187 [==============================] - 464s 2s/step - loss: 0.0182 - accuracy: 0.9957 - val_loss: 1.4994 - val_accuracy: 0.7531\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9933\n",
      "Epoch 00007: val_loss did not improve from 0.41263\n",
      "187/187 [==============================] - 446s 2s/step - loss: 0.0272 - accuracy: 0.9933 - val_loss: 1.6650 - val_accuracy: 0.8031\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9950\n",
      "Epoch 00008: val_loss did not improve from 0.41263\n",
      "187/187 [==============================] - 401s 2s/step - loss: 0.0186 - accuracy: 0.9950 - val_loss: 1.7147 - val_accuracy: 0.6375\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9913\n",
      "Epoch 00009: val_loss did not improve from 0.41263\n",
      "187/187 [==============================] - 381s 2s/step - loss: 0.0292 - accuracy: 0.9913 - val_loss: 2.1029 - val_accuracy: 0.6906\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9863\n",
      "Epoch 00010: val_loss did not improve from 0.41263\n",
      "187/187 [==============================] - 380s 2s/step - loss: 0.0368 - accuracy: 0.9863 - val_loss: 0.7364 - val_accuracy: 0.9219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Models: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [6:14:16<00:00, 7485.53s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "TrainedModels = []  \n",
    "histories = []      \n",
    "\n",
    "for model in tqdm(Models, desc=\"Training Models\"):\n",
    "    try:\n",
    "        # Fit the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.n // train_generator.batch_size,\n",
    "            epochs=num_epochs,\n",
    "            validation_data=valid_generator,\n",
    "            validation_steps=valid_generator.n // valid_generator.batch_size,\n",
    "            callbacks=[checkpointer, earlystopping]\n",
    "        )\n",
    "        \n",
    "        TrainedModels.append(model)\n",
    "        histories.append(history.history)  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training {model.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e8fddb1-e23d-43ce-8360-953961386107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " ...\n",
      " [1 0 1]\n",
      " [1 0 1]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for m in TrainedModels:\n",
    "    predicts = np.argmax(m.predict(test_generator), axis=1)\n",
    "    labels.append(predicts)\n",
    "\n",
    "# Ensemble with voting\n",
    "labels = np.array(labels)\n",
    "labels = np.transpose(labels, (1, 0))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e60943d2-7018-4b13-858e-e248e285d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1\n",
      " 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1\n",
      " 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 1 0 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 1 1\n",
      " 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
      " 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0\n",
      " 1 0 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0\n",
      " 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "labls = scipy.stats.mode(labels,axis=1, keepdims=False)[0]\n",
    "labls = np.squeeze(labls)\n",
    "\n",
    "original = np.asarray(test['diagnosis']).astype('int')  \n",
    "print (labls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5722863-fa7b-44bf-8a15-49c96a1d3691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9440677966101695\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       387\n",
      "           1       0.91      0.93      0.92       203\n",
      "\n",
      "    accuracy                           0.94       590\n",
      "   macro avg       0.94      0.94      0.94       590\n",
      "weighted avg       0.94      0.94      0.94       590\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = labls\n",
    "accuracy = accuracy_score(original, pred)\n",
    "print(accuracy)\n",
    "\n",
    "cm = confusion_matrix(original, pred)\n",
    "\n",
    "report = classification_report(original, pred, labels = [0,1])\n",
    "print(report)\n",
    "pyplot.figure(figsize = (5,5))\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7de0cb52-88d8-49ad-84a2-62a1b88d6e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: firstModel\\assets\n",
      "INFO:tensorflow:Assets written to: secondModel\\assets\n",
      "INFO:tensorflow:Assets written to: thirdModel\\assets\n"
     ]
    }
   ],
   "source": [
    "TrainedModels[0].save('firstModel')\n",
    "TrainedModels[1].save('secondModel')\n",
    "TrainedModels[2].save('thirdModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c0c95-866e-4973-a331-984895ce087c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
